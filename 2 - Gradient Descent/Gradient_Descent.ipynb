{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent<br>\n",
        "First, let's upload the modules that we need."
      ],
      "metadata": {
        "id": "zqiT3-eidKXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import cm\n",
        "import seaborn as sb\n",
        "from matplotlib.animation import FuncAnimation"
      ],
      "metadata": {
        "id": "fpo8W49i9MkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider this relation between two variables, $x$ and $y$."
      ],
      "metadata": {
        "id": "kj3aEi2kc7QR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnlmgSGrdBFD"
      },
      "outputs": [],
      "source": [
        "m = 2\n",
        "b = 0.5\n",
        "\n",
        "x = np.linspace(0,5,100)\n",
        "y = addrandomfluctuations(x,m,b,1)\n",
        "plt.plot(x,y,'bx')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Figure 2.1. A data set that forms a straight line.*"
      ],
      "metadata": {
        "id": "_hHHQ9z7WM3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Cost Function<br>\n",
        "The cost function describes is a measure of the error between the expected value and the measured value. The higher the error, the more it \"costs\" to correct the error. Thus, the goal is to minimize the error, and therefore, the cost function. <br>\n",
        "Consider the linear given data set plotted against the best fit line. For this example, the cost function, $J(\\theta_0,\\theta_1)$, is given by the expression<br>\n",
        "$J(\\theta_0,\\theta_1)=\\frac{1}{2n}\\sum_{i=1}^{n} [(\\theta_0 + \\theta_1 x_i)-y_i]^2$<br>\n",
        "where $(\\theta_0 + \\theta_1 x_i)$ is the equation of the best fit line described by its slope, $\\theta_1$, and y-intercept, $\\theta_0$, and  is the measured value at the $i^{th}$ data point. Thus, the term in the square brackets $(\\theta_0 + \\theta_1 x_i)-y_i$ corresponds to the difference between the best fit line and the measured values. When the cost function is minimized, we have the optimum value for the slope, $\\theta_1$, and y-intercept, $\\theta_0$, and the term  is small as it could be for each data point. In Figure 2.2, the term  is the value between each of the blue crosses and the value of the best fit line corresponding to $x_i$."
      ],
      "metadata": {
        "id": "RYrlqwXuWWdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x,y,'bx', markersize=10,linewidth=2)\n",
        "ylr = addrandomfluctuations(x,m,b,0)\n",
        "plt.plot(x,ylr,'r', linewidth=5)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis([0, 0.5, 0, 2])"
      ],
      "metadata": {
        "id": "wbjsPY_sWUwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Figure 2.2. The difference between the blue crosses and the red line at each data point, $x_i$, corresponds to the term $(\\theta_0 + \\theta_1 x_i)-y_i$.*"
      ],
      "metadata": {
        "id": "uJkE7K_gY5KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also display the first few data points by assigning the datasets to a table."
      ],
      "metadata": {
        "id": "lcQU8wiDZd0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "T = [[x,y]]\n",
        "print(tabulate(T,headers=['X Data','Y Data']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYGMp0c8ZCKh",
        "outputId": "2c5c2ed7-6a21-4660-cfa6-38993301a667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Data                                                              Y Data\n",
            "------------------------------------------------------------------  ------------------------------------------------------------------------\n",
            "[0.         0.05050505 0.1010101  0.15151515 0.2020202  0.25252525  [ 0.60083637  1.01738161  0.6138519   0.47774088  0.70154781  1.07200912\n",
            " 0.3030303  0.35353535 0.4040404  0.45454545 0.50505051 0.55555556    1.36997156  1.39393299  1.32924561  1.79057395  1.3304172   1.24065304\n",
            " 0.60606061 0.65656566 0.70707071 0.75757576 0.80808081 0.85858586    1.67572906  2.1805472   2.20429028  2.23837321  2.60263448  2.56303198\n",
            " 0.90909091 0.95959596 1.01010101 1.06060606 1.11111111 1.16161616    2.4178319   2.48587438  2.85837498  2.52048626  2.88232261  2.59133607\n",
            " 1.21212121 1.26262626 1.31313131 1.36363636 1.41414141 1.46464646    2.66967092  3.27516575  3.48012765  2.91773457  3.04692528  3.90386414\n",
            " 1.51515152 1.56565657 1.61616162 1.66666667 1.71717172 1.76767677    3.24692519  3.81893369  3.99887667  3.4600613   3.89368501  4.45292825\n",
            " 1.81818182 1.86868687 1.91919192 1.96969697 2.02020202 2.07070707    4.18248821  4.19987737  4.71571414  4.07617467  4.18945349  4.91003703\n",
            " 2.12121212 2.17171717 2.22222222 2.27272727 2.32323232 2.37373737    4.48802769  4.64436348  5.03115858  5.52595708  4.82979879  4.98406367\n",
            " 2.42424242 2.47474747 2.52525253 2.57575758 2.62626263 2.67676768    5.67859629  5.85050078  5.69538736  5.80408968  5.42642307  5.80269791\n",
            " 2.72727273 2.77777778 2.82828283 2.87878788 2.92929293 2.97979798    5.46985071  5.90568245  5.79809479  6.0692353   6.28632823  6.45609525\n",
            " 3.03030303 3.08080808 3.13131313 3.18181818 3.23232323 3.28282828    6.84825367  6.68547352  6.9437047   6.47811632  7.33017528  7.06587701\n",
            " 3.33333333 3.38383838 3.43434343 3.48484848 3.53535354 3.58585859    7.48595813  7.17952894  7.44087188  7.47365511  7.20537013  7.42582814\n",
            " 3.63636364 3.68686869 3.73737374 3.78787879 3.83838384 3.88888889    7.57868386  7.54950171  8.36170711  7.63078465  8.23606087  8.22117906\n",
            " 3.93939394 3.98989899 4.04040404 4.09090909 4.14141414 4.19191919    8.38695869  8.92415063  8.20296191  8.86545311  8.45220834  8.52271413\n",
            " 4.24242424 4.29292929 4.34343434 4.39393939 4.44444444 4.49494949    8.80101073  8.8105098   9.59542751  8.79137688  9.76844167  9.49111954\n",
            " 4.54545455 4.5959596  4.64646465 4.6969697  4.74747475 4.7979798     9.85461696 10.08382824 10.27743017 10.16314533 10.45125594 10.17858454\n",
            " 4.84848485 4.8989899  4.94949495 5.        ]                        10.10066985  9.83719827 10.00613636 10.33540879]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  (len(row) >= 1 and row[0] == SEPARATING_LINE)\n",
            "/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py:108: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  or (len(row) >= 2 and row[1] == SEPARATING_LINE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to understand what is happening here and what it means to our data when we minimize one of these parameters. Let's set the y-intercept to $\\theta_0=0.5$ and then vary the slope of the line, $\\theta_1$. While we vary the slope, we take a look at the cost function.<br>\n",
        "Watch this video and try to figure out what the value of $\\theta_1$ is that minimizes the cost function. The function *costfunction()* is defined at the end of this script and solves for the costfunction. To plot the result, we defined the plot_costfunction_2D() located at the end of this script. Here the y-intercept is fixed and the slope is varied. So, the red line is fixed at one end and just sweeps over an area on the graph eventually passing through all the data points neatly, which corresponds to when the cost function has a minimum value."
      ],
      "metadata": {
        "id": "S__6SubmceT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta0 = np.array([0.5])\n",
        "theta1 = np.arange(0,4,0.1)\n",
        "time = 0.2\n",
        "\n",
        "J = costfunction(x,y,theta0,theta1)\n",
        "plot_costfunction_2D(x,y,theta0,theta1,J,time)\n",
        "\n",
        "# fig,ax = plt.subplots(1,2,figsize=(8,8))\n",
        "# ph0, = ax[0].plot(x,ylr,'r')\n",
        "# ph1, = ax[1].plot(theta1,J[0,:],'bx')\n",
        "\n",
        "# def animate(i):\n",
        "#   ylr = addrandomfluctuations(x,theta1[:i],theta0,0)\n",
        "#   ph0.set_data(x, ylr)\n",
        "#   ph1.set_data(theta1[:i],J[0,:i])\n",
        "#   return [ph0,ph1]\n",
        "\n",
        "# ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
        "# plt.show()\n",
        "\n",
        "# ani.save('CostFunction.mp4', writer = 'ffmpeg', fps = 30)"
      ],
      "metadata": {
        "id": "2wkSvgCxbibk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Video 2.1. As the value of $\\theta_1$ varies from 0 to 4, we see the red line slowly becoming closer to the blue data points until it fits nicely. This corresponds to the minimum value of the cost function.*"
      ],
      "metadata": {
        "id": "VaPaQ2UL4Z_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe the same thing if we set the slope $\\theta_1=2$ and vary the y-intercept, $\\theta_0$.<br>\n",
        "Watch this video and try to figure out what the value of $\\theta_1$ is that minimizes the cost function. Here the slope is fixed and the y-intercept is varied. So, the red line slowly rises from a low y-intercept value to a high y-intercept value. At some value $\\theta_0$, the red line passes through all the points neatly, which corresponds to when the cost function has a minimum value."
      ],
      "metadata": {
        "id": "zFFrc7VY4g4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta0 = np.arange(-2,2,0.1)\n",
        "theta1 = np.array([2])\n",
        "time = 0.1\n",
        "\n",
        "J = costfunction(x,y,theta0,theta1)\n",
        "plot_costfunction_2D(x,y,theta0,theta1,J,time)"
      ],
      "metadata": {
        "id": "nEnq5Y644vzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Video 2.2. As the value of $\\theta_0$ varies from -2 to 2, we see the red line slowly becoming closer to the blue data points until it fits nicely. This corresponds to the minimum value of the cost function.*"
      ],
      "metadata": {
        "id": "BNomVpBl5aGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take a look at the full 3D image of the cost function. In other words, we plot the cost function, $J(\\theta_0,\\theta_1)$ against both $\\theta_0$ and $\\theta_1$, and then compare it with how the red line fits the data set.<br>\n",
        "First, let's take a look at where the minimum value of the cost function is. The black dot on the cost function graph depicts this minimum value."
      ],
      "metadata": {
        "id": "o0kmkzGj8xN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta0 = np.arange(-2,2,0.1)\n",
        "theta1 = np.arange(0,4,0.1)\n",
        "init0 = np.array([0.5])\n",
        "init1 = np.array([2])\n",
        "time = 0.1\n",
        "\n",
        "J = costfunction(x,y,theta0,theta1)\n",
        "plot_costfunction_3D(x,y,theta0,theta1,J,init0,init1,time)"
      ],
      "metadata": {
        "id": "MxLCDxrucBZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Video 2.3. The location of the minimum value and the best fit line for the given data set.*"
      ],
      "metadata": {
        "id": "k_RIsUwd2CV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we fix the y-intercept, $\\theta_0$, and allow the slope, $\\theta_1$ to vary. The black dot on the 3D plot of the cost function corresponds to the various values of the y-intercept and the slope."
      ],
      "metadata": {
        "id": "yK09V8ch2H7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init0 = np.arange(-2,2,0.1)\n",
        "init1 = np.array([2])\n",
        "time = 0.1\n",
        "\n",
        "J = costfunction(x,y,theta0,theta1)\n",
        "plot_costfunction_3D(x,y,theta0,theta1,J,init0,init1,time)"
      ],
      "metadata": {
        "id": "Q1frypF6luzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Video 2.4. The various values of the slope, $\\theta_1$, with a fixed y-intercept, $\\theta_0$ in relation to the fitting line.*"
      ],
      "metadata": {
        "id": "YTti2Ydn2Wtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now vary the y-intercept, $\\theta_0$, and fix the slope, $\\theta_1$ to a specific value. The black dot on the 3D plot of the cost function corresponds to the various values of the y-intercept and the slope."
      ],
      "metadata": {
        "id": "6dehIU8G2gMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init0 = np.array([0.5])\n",
        "init1 = np.arange(0,4,0.1)\n",
        "time = 0.1\n",
        "\n",
        "J = costfunction(x,y,theta0,theta1)\n",
        "plot_costfunction_3D(x,y,theta0,theta1,J,init0,init1,time)"
      ],
      "metadata": {
        "id": "NER8GE71mLqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Video 2.5. The various values of the y-intercept, $\\theta_0$, with a fixed slope, $\\theta_1$, in relation to the fitting line.*"
      ],
      "metadata": {
        "id": "YEO6LHl02okm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Derivative of the Cost Function\n",
        "How do we determine the minimum value of the cost function? We use gradient descent to determine the fastest way to the minimum value from a given set of initial parameters or initial guesses. To perform gradient descent, we need to take the derivative of the cost function with respect to the parameters, $\\theta_0$ and $\\theta_1$. We get the following expressions:<br>\n",
        "$\\frac{\\partial J(\\theta_0,\\theta_1)}{\\partial \\theta_0} = \\frac{1}{n} \\sum_{i=1}^{n} [(\\theta_0 + \\theta_1 x_i)-y_i]$ and $\\frac{\\partial J(\\theta_0,\\theta_1)}{\\partial \\theta_1} = \\frac{1}{n} \\sum_{i=1}^{n} [(\\theta_0 + \\theta_1 x_i)-y_i]x_i$.<br>\n",
        "These derivatives point towards the greatest change in the cost function along the direction of the parameters. Thus, in one iteration, we get a new guess for the optimum value by subtracting these values from the initial guesses. In other words,<br>\n",
        "$\\theta_0 - \\alpha \\frac{\\partial J(\\theta_0,\\theta_1)}{\\partial \\theta_0} \\rightarrow \\theta_0$ and $\\theta_1 - \\alpha \\frac{\\partial J(\\theta_0,\\theta_1)}{\\partial \\theta_1} \\rightarrow \\theta_1$.<br>\n",
        "Using those new values, we get a new set of gradients, which we then subtract again from these new guesses to get a third guess. We do this iteratively until the gradient becomes very small or until after a specified number of iterations. The implementation of the gradient descent can be seen in the *gradient_descent()* function at the end of the script."
      ],
      "metadata": {
        "id": "nTdPnR7F2wvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 51\n",
        "learning_rate = 0.15\n",
        "time = 0.1\n",
        "\n",
        "init0 = -2  # possible values -2,2,0.1\n",
        "init1 = 0   # possible values 0,4,0.1\n",
        "\n",
        "Jcost = gradient_descent(x,y,init0,init1,iter,learning_rate,time)"
      ],
      "metadata": {
        "id": "GDo2Arh40BSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Video 2.5. Using gradient descent to determine the optimum value of the cost function.*"
      ],
      "metadata": {
        "id": "PzN2WFC29yjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the cost function and see how it decreases per iteration."
      ],
      "metadata": {
        "id": "FCeOWYgR95BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Jcost,linewidth=3)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Cost Function, J')"
      ],
      "metadata": {
        "id": "LKmmeHAK9842"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Figure 2.3. The decrease of the cost function is seen with the number of iterations or epochs.*"
      ],
      "metadata": {
        "id": "_S2pJVVtAN3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Learning Rate\n",
        "The basic algorithm for the gradient descent can be written as<br>\n",
        "$iterate \\{ \\theta_i - \\alpha \\frac{\\partial J(\\theta_0,\\theta_1)}{\\partial \\theta_i} \\rightarrow \\theta_i \\}_{i=0,1}$.<br>\n",
        "You might notice that there is a parameter, $\\alpha$. This parameter is called a learning rate. It's a parameter that when properly chosen, the function converges to the optimum value quickly. Otherwise, the convergence will be too slow or it might not converge at all. Using the same code we used earlier, try to change the learning rate to a smaller value and then to a larger value to see what happens.<br>\n",
        "In choosing the learning rate, you want a relatively smooth decrease of the cost function with the number of iterations."
      ],
      "metadata": {
        "id": "TjODO0LaAPDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to be used for this workbook"
      ],
      "metadata": {
        "id": "E-QMG7oCdEBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define all the functions that we are going to use."
      ],
      "metadata": {
        "id": "dt0GtSp7c_k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## function that adds random values to the vertical axis\n",
        "## so that the data set looks random\n",
        "def addrandomfluctuations(x,m,b,fluctuationsize):\n",
        "  import random\n",
        "\n",
        "  yrand = np.zeros(len(x))\n",
        "  for i in range(len(x)):\n",
        "    yrand[i] = m*x[i] + b + fluctuationsize*(random.random()-0.5)\n",
        "\n",
        "  return yrand"
      ],
      "metadata": {
        "id": "TFZsxBfGc-OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## function that computes the cost function\n",
        "def costfunction(x,y,th0,th1):\n",
        "  n = len(y)\n",
        "\n",
        "  J = np.zeros([len(th0),len(th1)])\n",
        "  for i in range(len(th0)):\n",
        "    for j in range(len(th1)):\n",
        "      J[i,j] = (1/(2*n))*np.sum(np.power((th0[i] + th1[j]*x - y),2))\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "siGLKoD4dM1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_costfunction_2D(x,y,th0,th1,J,time):\n",
        "\n",
        "  for i in range(len(th0)):\n",
        "    for j in range(len(th1)):\n",
        "      fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
        "      plt.axis([0,5,0,12])\n",
        "      ylr = addrandomfluctuations(x,th1[j],th0[i],0)\n",
        "      ax[0].plot(x,ylr,'r', linewidth=5)\n",
        "      ax[0].plot(x,y,'bx', markersize=10,linewidth=2)\n",
        "      plt.xlabel('x')\n",
        "      plt.ylabel('y')\n",
        "\n",
        "      if th1.shape[0] == 1:\n",
        "        ax[1].plot(th0[:i],J[:i,j],'bx', markersize=10,linewidth=2)\n",
        "        plt.xlabel(r'$\\theta_0$')\n",
        "        plt.ylabel('Cost Function, J')\n",
        "        plt.axis([-2,2,0,4])\n",
        "      else:\n",
        "        ax[1].plot(th1[:j],J[i,:j],'bx', markersize=10,linewidth=2)\n",
        "        plt.xlabel(r'$\\theta_1$')\n",
        "        plt.ylabel('Cost Function, J')\n",
        "        plt.axis([0,4,0,20])\n",
        "\n",
        "      plt.show()\n",
        "      plt.pause(time)"
      ],
      "metadata": {
        "id": "gUoXP0v16eV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_costfunction_3D(x,y,theta0,theta1,J,th0,th1,time):\n",
        "\n",
        "  for i in range(len(th0)):\n",
        "    for j in range(len(th1)):\n",
        "      fig = plt.figure(figsize=(16,8))\n",
        "      plt.axis([0,5,0,12])\n",
        "      ax = fig.add_subplot(1,2,1)\n",
        "      ylr = addrandomfluctuations(x,th1[j],th0[i],0)\n",
        "      ax.plot(x,ylr,'r', linewidth=5)\n",
        "      ax.plot(x,y,'bx', markersize=10,linewidth=2)\n",
        "      plt.xlabel('x')\n",
        "      plt.ylabel('y')\n",
        "\n",
        "      ax = fig.add_subplot(1,2,2, projection = '3d')\n",
        "      surf = ax.plot_wireframe(theta0,theta1,(J),cmap=cm.Spectral,antialiased=True)\n",
        "      # Jcost = (1/(2*len(y)))*np.sum(np.power((th0[i] + th1[j]*x - y),2))\n",
        "      scatter2 = ax.scatter(th0[i],th1[j],J[i,j],c='k',marker='o',linewidth=5)\n",
        "      ax.view_init(30, 30)\n",
        "      ax.set_xlabel(r'$\\theta_0$')\n",
        "      ax.set_ylabel(r'$\\theta_1$')\n",
        "      ax.set_zlabel(r'Cost Function, J')\n",
        "\n",
        "      plt.pause(time)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "DLlTrKwLf9Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x,y,init0,init1,iter,a,time):\n",
        "  t0 = np.zeros([iter])\n",
        "  t1 = np.zeros([iter])\n",
        "  gradJcost0 = np.zeros([iter])\n",
        "  gradJcost1 = np.zeros([iter])\n",
        "  Jcost = np.zeros([iter])\n",
        "  t0[0] = init0\n",
        "  t1[0] = init1\n",
        "\n",
        "  n = len(y)\n",
        "  theta0 = np.arange(-2,2,0.1)\n",
        "  theta1 = np.arange(0,4,0.1)\n",
        "  J = costfunction(x,y,theta0,theta1)\n",
        "\n",
        "  for i in range(iter-1):\n",
        "    print(t0[i])\n",
        "\n",
        "    fig = plt.figure(figsize=(16,8))\n",
        "    # plt.axis([0,5,0,12])\n",
        "    ax = fig.add_subplot(1,2,1)\n",
        "    ylr = addrandomfluctuations(x,t1[i],t0[i],0)\n",
        "    ax.plot(x,ylr,'r', linewidth=5)\n",
        "    ax.plot(x,y,'bx', markersize=10,linewidth=2)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "\n",
        "    ax = fig.add_subplot(1,2,2,projection = '3d')\n",
        "    surf = ax.plot_wireframe(theta0,theta1,(J),cmap=cm.Spectral,antialiased=True)\n",
        "    Jcost[i] = (1/(2*len(y)))*np.sum(np.power((t0[i] + t1[i]*x - y),2))\n",
        "    scatter2 = ax.scatter(t0[i],t1[i],Jcost,c='k',marker='o',linewidth=5)\n",
        "    ax.view_init(50, 10)\n",
        "    ax.set_xlabel(r'$\\theta_0$')\n",
        "    ax.set_ylabel(r'$\\theta_1$')\n",
        "    ax.set_zlabel(r'Cost Function, J')\n",
        "\n",
        "    plt.pause(time)\n",
        "\n",
        "    # Solve for the derivative\n",
        "    gradJcost0[i] = (1/n)*sum(t0[i] + t1[i]*x - y)\n",
        "    gradJcost1[i] = (1/n)*sum( (t0[i] + t1[i]*x - y)*x )\n",
        "\n",
        "    # Get new values of the parameters\n",
        "    t0[i+1] = t0[i] - a*gradJcost0[i]\n",
        "    t1[i+1] = t1[i] - a*gradJcost1[i]\n",
        "\n",
        "  plt.show()\n",
        "  return Jcost"
      ],
      "metadata": {
        "id": "pftgoeLu1_Tb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}